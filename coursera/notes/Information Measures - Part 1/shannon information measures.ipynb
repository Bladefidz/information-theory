{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shannon's Information Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four types of Shannon's Information Measures:\n",
    "\n",
    "- Entropy\n",
    "- Conditional entropy\n",
    "- Mutual information\n",
    "- Conditional mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 2.13** The entropy $H(X)$ of a random variable $X$ is defined as\n",
    "\n",
    "  $$H(X) = -\\sum_x\\, p(x)\\,log\\, p(x)$$\n",
    "  \n",
    "- Convention: summation is taken over $S_X$\n",
    "- When the base of the logarithm is $\\alpha$, write $H(X)$ as $H_{\\alpha}(X)$.\n",
    "- Entropy measures the uncertainty of a discrete random variable.\n",
    "- The unit for entropy is:\n",
    "  $$\\text{bit} \\qquad \\text{if}\\, \\alpha=2\\\\\n",
    "  \\text{nat} \\qquad \\text{if}\\, \\alpha=e\\\\\n",
    "  D\\text{-it} \\qquad \\text{if}\\, \\alpha=D$$\n",
    "- A bit in information theory is **different** from a bit in computer science.\n",
    "\n",
    "**Remark** $H(X)$ depends only on the distribution of $X$ but not on the actual values taken by $X$, hence also write $H(p_X)$.\n",
    "\n",
    "**Example** Let $X$ and $Y$ be random variable with $\\mathcal{X} = \\mathcal{Y} = \\{0,1\\}$, and let\n",
    "\n",
    "  $$p_X(0) = 0.3,\\qquad p_X(1) = 0.7$$\n",
    "\n",
    "and\n",
    "\n",
    "  $$p_Y(0) = 0.7,\\qquad p(1) = 0.3$$\n",
    "  \n",
    "Although $P_X \\neq P_Y$, $H(X)=H(Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy as Expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convention\n",
    "\n",
    "  $$E_g(X) = \\sum_x p(x) g(x)$$\n",
    "  \n",
    "  where summation is over $S_X$.\n",
    "  \n",
    "- Linearity\n",
    "\n",
    "  $$E[f(X) + g(X)] = Ef(X) + Eg(X)$$\n",
    "  \n",
    "- Can write\n",
    "\n",
    "  $$H(X) = -Elogp(X) = -\\sum_xp(x)\\,log\\,p(x)$$\n",
    "  \n",
    "- In probability theory, when $E_g(X)$ is considered, usually $g(x)$ depends only on the value of $x$ but not on $p(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Entropy Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For $0 \\leq \\gamma \\leq 1$, define the binary entropy function\n",
    "\n",
    "  $$h_b(\\gamma) = -\\gamma\\, log\\, \\gamma - (1-\\gamma)log\\, (1-\\gamma)$$\n",
    "  \n",
    "  with the convention $0\\,log\\,0 = 0$, as by L'Hospital rule:\n",
    "  \n",
    "  $$lim_{\\alpha \\rightarrow 0} \\alpha\\, log\\, \\alpha = 0$$\n",
    "  \n",
    "- For $X \\in \\{\\gamma, 1-\\gamma\\}$,\n",
    "\n",
    "  $$H(X)=h_b(\\gamma)$$\n",
    "  \n",
    "- $h_b(\\gamma)$ achieves the maximum value 1 when $\\gamma = \\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Binary-Entropy](images/binary-entropy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider tossing a coin with\n",
    "\n",
    "  $$p(H) = \\gamma\\, \\text{and}\\, p(T)=1-\\gamma$$\n",
    "  \n",
    "Then $h_b{\\gamma}$ measures the amount of uncertainty in the outcome of the toss.\n",
    "\n",
    "  - When $\\gamma = 0 \\, \\text{or}\\, 1$, the coin is *deterministic* and $h_b(\\gamma)=0$. This is consistent with our intuition because for such cases we need 0 bit to convey the outcome.\n",
    "  - When $\\gamma=0.5$, the coin is *fair* and $h_b(\\gamma)=1$. This is consistent with our intuition because we need 1 bit to convey the outcome.\n",
    "  - When $\\gamma \\notin \\{0,0.5,1\\}$, $0<h_b(\\gamma)<1$, i.e., the uncertainty about the outcome is somewhere between 0 and 1 bit.\n",
    "  - This interpretation will be justified in terms of the source coding theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 2.14** The joint entropy $H(X,Y)$ of a pair of random variables $X$ and $Y$ is defined as\n",
    "\n",
    "  $$H(X,Y)=-\\sum_{x,y}\\, p(x,y)\\, log\\, p(x,y) = -E\\, log\\, p(X,Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 2.15** For random variables $X$ and $Y$, the conditional entropy $Y$ given $X$ is defined as:\n",
    "\n",
    "  $$H(Y|X)=-\\sum_{x,y}\\, p(x,y)\\, log\\, p(y|x)=-E\\,log\\,p(Y|X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write,\n",
    "\n",
    "  $$\\eqalign{\n",
    "    H(Y|X) &= -\\sum_{z,y}p(x,y)\\,log\\,p(y|x)\\\\\n",
    "           &= -\\sum_z\\sum_yp(x)p(y|x)\\,log\\,p(y|x)\\\\\n",
    "           &= \\sum_zp(x)\\big[-\\sum_yp(y|x)\\,log\\,p(y|x)\\big]\n",
    "  }$$\n",
    "  \n",
    "- The inner sum is the entropy of $Y$ conditioning on a fixed $x \\in S_X$.\n",
    "\n",
    "- Denoting the inner sum by $H(Y|X=x)$, we have:\n",
    "\n",
    "  $$H(Y|X)=\\sum_xp(x)H(Y|X=x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similarly,\n",
    "\n",
    "  $$H(Y|X,Z)=\\sum_zp(z)H(Y|X,Z=z)$$\n",
    "  \n",
    "  where\n",
    "\n",
    "  $$H(Y|X,Z=z)=-\\sum_{x,y}p(x,y|z)\\,log\\,p(y|x,z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposition 2.16**\n",
    "\n",
    "  $$H(X,Y)=H(X)+H(Y|X)$$\n",
    "  \n",
    "and\n",
    "\n",
    "  $$H(X,Y)=H(Y)+H(X|Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof**\n",
    "\n",
    "Consider\n",
    "\n",
    "  $$\\eqalign{\n",
    "    H(X,Y) &= -E\\,log\\,p(X,Y)\\\\\n",
    "           &= -E\\,log[p(X)p(Y|X)]\\\\\n",
    "           &= -E\\,log\\,p(X)-E\\,log\\,p(Y|X)\\\\\n",
    "           &= H(X) + H(Y|X)\n",
    "  }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 2.17** For random variables $X$ and $Y$, the mutual information between $X$ and $Y$ is defined as:\n",
    "\n",
    "  $$I(X;Y)=\\sum_{x,y}p(x,y)\\,log\\,\\frac{p(x,y)}{p(x)p(y)}=E\\,log\\.\\frac{p(X,Y)}{p(X)p(Y)}$$\n",
    "  \n",
    "**Remark** $I(X;Y)$ is symmetrical in $X$ and $Y$.\n",
    "\n",
    "**Remark** Alternatively, we can write\n",
    "\n",
    "  $$I(X;Y)=\\sum_{x,y}p(x,y)\\,log\\,\\frac{p(x,y)}{p(x)p(y)}=\\sum_{x,y}p(x,y)\\,log\\,\\frac{p(x|y)}{p(x)}=E\\,log\\,\\frac{p(X|Y)}{p(X)}$$\n",
    "  \n",
    "However, it is not apparent from this form that $I(X;Y)$ is simmetrical in $X$ and $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposition 2.18** The mutual information between a random variable $X$ and itself is equal to the entropy of $X$, i.e., $I(X;X)=H(X)$\n",
    "\n",
    "**Proof**\n",
    "\n",
    "  $$\\eqalign{\n",
    "    I(X;X) &= E\\,log\\,\\frac{p(X,X)}{p(X)p(X)}\\\\\n",
    "           &= E\\,log\\,\\frac{p(X)}{p(X)p(X)}\\\\\n",
    "           &= -E\\,log\\,p(X)\\\\\n",
    "           &= H(X)\n",
    "  }$$\n",
    "  \n",
    "**Remark** The entropy of $X$ is sometimes called the *self-information* of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposition 2.19**\n",
    "\n",
    "  $$I(X;Y)=H(X)-H(X|Y)\\\\\n",
    "  I(X;Y)=H(Y)-H(Y|X)$$\n",
    "\n",
    "and\n",
    "\n",
    "  $$I(X;Y)=H(X)+H(Y)-H(X,Y)$$\n",
    "  \n",
    "provided that all the entropies and conditional entropies are finitie.\n",
    "\n",
    "**Remark**\n",
    "\n",
    "  $$I(X;Y)=H(X)+H(Y)-H(X,Y)$$\n",
    "\n",
    "is analogous to\n",
    "\n",
    "  $$\\mu(A\\cap B)=\\mu(A)+\\mu(B)-\\mu(A\\cup B)$$\n",
    "  \n",
    "where $\\mu$ is a set-additive function and $A$ and $B$ are sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![information-diagram](images/information-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 2.20** For random variables $X,\\,Y\\,\\text{and}\\,Z$, the mutual information between $X$ and $Y$ conditioning on $Z$ is defined as:\n",
    "\n",
    "  $$I(X;Y|Z)=\\sum_{x,y,z}p(x,y,z)\\,log\\,\\frac{p(x,y|z)}{p(x|z)p(y|z)}=E\\,log\\,\\frac{p(X,Y|Z)}{p(X|Z)p(Y|Z)}$$\n",
    "  \n",
    "**Remark** $I(X;Y|Z)$ is simmetrical in $X$ and $Y$.\n",
    "\n",
    "Similar to entropy, we have\n",
    "\n",
    "  $$I(X;Y|Z)=\\sum_xp(z)I(X;Y|Z=z)$$\n",
    "  \n",
    "where\n",
    "\n",
    "  $$I(X;Y|Z=z)=\\sum_{x,y}p(x,y|z)\\,log\\,\\frac{p(x,y|z)}{p(x|z)p(y|z)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposition 2.21** The mutual information between a random variable $X$ and itself conditioning on a random variable $Z$ is equal to the conditional entropy of $X$ given $Z$, i.e., $I(X;X|Z)=H(X|Z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposition 2.22**\n",
    "\n",
    "  $$I(X;Y|Z) = H(X|Z)-H(X|Y,Z)\\\\\n",
    "  I(X;Y|Z) = H(Y|Z)-H(Y|X,Z)$$\n",
    "  \n",
    "and\n",
    "\n",
    "  $$I(X;Y|Z) = H(X|Z) + H(Y|Z) - H(X,Y|Z)$$\n",
    "  \n",
    "provided that all the conditional entropies are finite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** All Shannon's information measures are special cases of conditional mutual iformation. Let $\\Phi$ denote a random variable that takes a constant value. Then\n",
    "\n",
    "  $$\\eqalign{\n",
    "    H(X) &= I(X;X|\\Phi)\\\\\n",
    "    H(X|Z) &= I(X;X|Z)\\\\\n",
    "    I(X;Y) &= I(X;Y|\\Phi)\n",
    "  }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
