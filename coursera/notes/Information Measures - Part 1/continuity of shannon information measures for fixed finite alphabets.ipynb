{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuity of Shannon Information Measures for Fixed Finite Alphabets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Alphabet vs Countable Alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All Shannon's information measures are contnious when the alphabets are fixed and finite.\n",
    "\n",
    "- For countable alphabets, Shannon's information measures are everywhere discontinous.\n",
    "\n",
    "- To probe further, see problems 28, 29, 30, 31, and\n",
    "\n",
    "  [On the discontinuity of the Shannon information measures](http://iest2.ie.cuhk.edu.hk/~whyeung/publications/discontinuity.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 2.23** Let $p$ and $q$ be two probability distributions on a common alphabet $\\mathcal{X}$. The variational distance between $p$ and $q$ is defined as\n",
    "\n",
    "  $$V(p,q) = \\sum_{x\\in \\mathcal{X}}|p(x)-q(x)|$$\n",
    "  \n",
    "The entropy functions is countinous at $p$ if\n",
    "\n",
    "  $$lim_{p'\\rightarrow p}H(p') = H\\big(lim_{p'\\rightarrow p}p'\\big) = H(p)$$\n",
    "  \n",
    "or equivalent, for any $\\epsilon > 0$, there exists $\\delta > 0$ such that\n",
    "\n",
    "  $$|H(p)-H(q)|<\\epsilon$$\n",
    "  \n",
    "for all $q \\in P_{\\mathcal{X}}$ satisfying\n",
    "\n",
    "  $$V(p,q)<\\delta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let $\\mathcal{X}=\\{1,2,...,\\}$, a countably infinite alphabet.\n",
    "\n",
    "- Let $P_X=\\{1,0,0,...\\}$, and let\n",
    "\n",
    "  $$P_{X_n}=\\Big\\{1-\\frac{1}{\\sqrt{log\\,n}},\\frac{1}{n\\sqrt{log\\,n}},...,\\frac{1}{n\\sqrt{log\\,n}},0,0,...\\Big\\}$$\n",
    "  \n",
    "- A $n \\rightarrow \\infty$,\n",
    "\n",
    "  $$V(P_X,P_{X_n})=\\sum_i|P_X(i)-P_{X_n}(i)|=\\frac{2}{\\sqrt{log\\,n}}\\rightarrow 0$$\n",
    "  \n",
    "- However,\n",
    "\n",
    "  $$H\\Big(lim_{n\\rightarrow \\infty}P_{X_n}\\Big)=H(P_X)=0$$\n",
    "  \n",
    "  but,\n",
    "  \n",
    "  $$lim_{n\\rightarrow \\infty}H(P_{X_n}) = \\infty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
